1 机器学习的算法大致分为两类
	** 监督者算法(supervised)
		给出的数据中已经有明显的结构标记，即已经确定每个数据的特点，机器根据这些特点去做出预测或者分类

		** 回归问题 regression
			根据给定的数据回答对将来的预测
			比如商品一段时间的销售报表，预测下一阶段的销售情况
			** 线性回归算法:建立线模型，解决回归问题的算法
				代价函数 cost function
					: 可以使得建模误差的平方和能够最小的模型参数，就叫代价函数，也叫平方误差函数，是解决线性回归问题的一个很合理的方法


		** 分类问题:classfication
			根据给定的数据的标签，将更多数据进行分类
			比如分析被标记为垃圾邮件的邮件的特点，将来对邮件进行自主分类


	**无监督着算法(unsupervised)
		给出的数据中无明显结构，需要机器自主分析得出数据的结构
		-> 聚类算法 	
			将相同的数据按照一定特点去分组分类
				比如谷歌的新闻会按照新闻的内容去将其分类


2 梯度下降算法
	常用于最小化代价函数，在下降的过程中，会用到同步更新，即将所有的 parameters 的值都更新之后，再去计算代价函数的值，这就是一次下降。

	parameter1 := parameter1 - a(下降速率) * 微粉项（就是该点的导数式）* 代价函数

	根据上述公式可以得出两个结论 
		-> 如果一开始就在局部最小值，那么就不在下降，因为最小值的导数式的值为0(一条水平线)

		-> 下降的过程中导数式越来越小（切线越来越平滑），所以梯度下降会自然的减速直到达到局部最小值

3 多元线性回归
	
假设一个回归问题有多个特征量(feature)
即 h(x) = a0 + a1*x1 + a2*x2...an*xn

那么这个式子可以简化为
h(x) = a0 * x0(假设x0=1)+ a1*x1.....an*xn
也就是
	AtX(A矩阵的转置矩阵乘以X矩阵，最后会得到一个数)

4 当多个特征(x0,x1,x2)的取值范围差异过多时，会增大梯度下降算法的复杂度
	这时候就要使用特征缩放 (feature scaling) 来趋近其取值

	一般来说， -3 < xj < 3 都可以接受

	使用 Mean normalization 来进行缩放

	x1 = (x1 - u1) / s1
	-> u1 是 x1 的平均值
	-> s1 是 x1 的范围（最大值减最小值） 
	特征缩放不用太精确，其目的只是为了让梯度下降更快一点

5 标准方程法 Normal Equation

使用标准方程法就不需要归一化特征了。

求最适合 threate的方程

sita = (XT X)-1 XT y

X 是给定数据集的矩阵

X = [

		1, 460, 330, 220
		1, 550, 260, 990
		...
	]

计算（X的转置乘以X）的逆乘乘以X的转置乘以y

Octave 写法 ； pinv(x'*x)*x'*y

6 在标准方程法中，有可能出现矩阵不可逆的情况
	-> 有冗余的 feature
		比如，两个 feature 之间存在线性关系

	-> feature 数量太多，比如比数据集多的时候


7 对于分类的问题用逻辑回归算法

8 逻辑回归算法公式

h0(x) = g(sitasup(T) * x)
g(z) = 1/(1 + esup(-z)