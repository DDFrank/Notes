== Redis 的高可用

* 尽量保证数据不丢失

使用 *AOF* 和 *RDB* 保证

* 尽量保证服务不中断

增加副本冗余量，将一份数据同时保存在多个实例上。即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。

== Redis 如何保证副本数据的一致性

* Redis提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式

* *读操作*：主库、从库都可以接收

* *写操作*：首先到主库执行，然后，主库将写操作同步给从库。

TIP: 如果从库也可以接收写操作，那么写操作可能落到多个实例上，会涉及到加锁、实例间协商是否完成修改等一系列操作，但这会带来巨额的开销，当然是不太能接受的

== Redis 如何实现 主从之间的数据同步

=== 主从库间如何进行第一次同步

启动多个Redis实例的时候，它们相互之间就可以通过replicaof（Redis 5.0之前使用slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步

* 主从库间建立连接、协商同步的过程，主要是为全量复制做准备。 在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了

** 具体来说,从库给主库发送psync命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync命令包含了主库的 *runID* 和复制进度 *offset* 两个参数

*** *runID*，是每个Redis实例启动时都会自动生成的一个随机ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的runID，所以将runID设为 "?"

*** *offset*，此时设为-1，表示第一次复制

** 主库收到psync命令后，会用 *FULLRESYNC* 响应命令带上两个参数：主库*runID*和主库目前的复制进度*offset*，返回给从库。*FULLRESYNC* 表示第一次复制采用的全量复制，所以主库会把当前的所有数据复制给从库

** 从库收到响应后，会记录下这两个参数

* 主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的RDB文件

** 具体来说，主库执行bgsave命令，生成RDB文件，接着将文件发给从库

** 从库接收到RDB文件后，会先清空当前数据库，然后加载RDB文件

TIP: 这是因为从库在通过replicaof命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空

** 在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求,但是，这些请求中的写操作并没有记录到刚刚生成的RDB文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 *replication buffer* ，记录 *RDB* 文件生成后收到的所有写操作

* 最后, 主库会把第二阶段执行过程中新收到的写命令，再发送给从库

** 具体来说，就是 主库完成RDB文件发送后，就会把此时 *replication buffer* 中的修改操作发给从库，从库再重新执行这些操作

=== 主从级联模式分担全量复制时的主库压力

* 一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成RDB文件和传输RDB文件

* 如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于fork子进程生成RDB文件，进行数据全量同步。fork这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。此外，传输RDB文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力

* 可以通过 "主-从-从" 模式将主库生成RDB和传输RDB的压力，以级联的方式分散到从库上

* 简单来说，我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系

TIP: 也是使用 *replicaof* 命令, 格式一致

* 一旦主从库完成了全量复制, 它们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为基于长连接的命令传播，可以避免频繁建立连接的开销

=== 主从库间网络中断的应对策略

* Redis 2.8之前，如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重新进行一次全量复制，开销非常大

* Redis 2.8开始，网络断了之后，主从库会采用增量复制的方式继续同步, 也就是只同步主从库网络中断期间主库收到的命令

==== 增量复制是如何实现的

* 主从库断连后，主库会把断连期间收到的写操作命令，写入 *replication buffer*，同时也会把这些操作命令也写入 *repl_backlog_buffer* 这个缓冲区

* *repl_backlog_buffer* 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置

* 主库在 *repl_backlog_buffer* 的偏移量称为 *master_repl_offset*, 从库在 *repl_backlog_buffer* 的偏移量称为 *slave_repl_offset*

* 主从库的连接恢复之后，从库首先会给主库发送 *psync* 命令，并把自己当前的 *slave_repl_offset* (也就是 psync 的第二个参数)发给主库，主库会判断自己的 *master_repl_offset* 和 *slave_repl_offset* 之间的差距

* 此时，主库只用把 *master_repl_offset* 和 *slave_repl_offset* 之间的命令操作同步给从库就行

IMPORTANT: 因为 *repl_backlog_buffer* 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致

==== repl_backlog_size 参数的设置

这个参数的大小和缓冲空间的计算方式有关

缓冲空间的计算公式是：缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即repl_backlog_size = 缓冲空间大小 * 2，这也就是repl_backlog_size的最终值

举个例子，如果主库每秒写入2000个操作，每个操作的大小为2KB，网络每秒能传输1000个操作，那么，有1000个操作需要缓冲起来，这就至少需要2MB的缓冲空间。否则，新写的命令就会覆盖掉旧操作了。为了应对可能的突发压力，我们最终把repl_backlog_size设为4MB

== Redis 如何保证数据不中断

当主库挂掉时，读的操作还可以向从库请求，但是写的操作就无法进行了，这个时候，就会面临三个问题

* 主库真的挂了吗？

* 该选择哪个从库作为主库？

* 怎么把新主库的相关信息通知给从库和客户端呢？

== Redis 的哨兵机制

=== 基本流程

* 哨兵其实就是一个运行在特殊模式下的Redis进程，主从库实例运行的同时，它也在运行

* 哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知

==== 监控

监控是指哨兵进程在运行时，周期性地给所有的主从库发送PING命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的PING命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的PING命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。

==== 选主

主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库

==== 通知

在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 *replicaof* 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上

=== 哨兵如何判断主库已经下线

* 哨兵对主库的下线状态的判断有 *主观下线* 和 *客观下线* 两种

* 哨兵进程会使用PING命令检测它自己和主、从库的网络连接情况，用来判断实例的状态

* 如果检测的是从库，那么，哨兵简单地把它标记为 *主观下线* 就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断

* 如果检测的是主库，那么，哨兵还不能简单地把它标记为 *主观下线* ，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判了，其实主库并没有故障

* 多个哨兵节点组成集群，如果大多数的哨兵都判断主库已经 *主观下线*，主库就会被标记为 "客观下线", 此时，就会触发哨兵开始主从切换流程

=== 哨兵如何选主

* 基本流程是，先从多个从库中，按照一定条件筛选掉不符合的从库，再按照一定的规则，给剩下的从库逐一打分, 得分最高的的从库选为主库

* 筛选的条件是 从库仍在运行，且网络状况良好

判断依据是配置项 *down-after-milliseconds* ，主从库断连的最大连接超时时间

也就是说假如 *down-after-milliseconds* 毫秒内，主从节点都没有联系上，那么就认为主从节点断连了

如果发生断连的次数发生了10次，那么就判断网络状况并不好

* 打分有3轮，假如有一轮中，有从库的得分最高，那么它就是主库了，选主过程就结束了。如果没有出现得分最高的库，就继续下一轮

** 优先级最高的从库

用户可以通过 *slave-priority* 配置项，给不同的从库设置不同优先级。

** 和旧主库同步程度最接近的从库得分高

主从库同步时有个命令传播的过程。在这个过程中，主库会用 *master_repl_offset* 记录当前的最新写操作在 *repl_backlog_buffer* 中的位置，而从库会用 *slave_repl_offset* 这个值记录当前的复制进度

那么最接近 *master_repl_offset* 的 *slave_repl_offset* 的从库就会被选为主库

** ID号小的从库得分高

每个实例都会有一个ID，这个ID就类似于这里的从库的编号。目前，Redis在选主库时，有一个默认的规定：在优先级和复制进度都相同的情况下，ID号最小的从库得分最高，会被选为新主库


== Redis 的哨兵集群

* 配置哨兵集群的时候，只需要配置主库的ip和端口，并不需要配置其它信息

[source, shell]
----
sentinel monitor <master-name> <ip> <redis-port> <quorum> 
----

那么哨兵之间是怎么知道彼此的地址和从库的地址呢?

=== 基于pub/sub机制的哨兵集群组成

* 哨兵实例之间可以相互发现，是通过在 主库上进行 pub/sub 来实现的

* 在主从集群中，主库上有一个名为 *__sentinel__:hello* 的频道，不同哨兵就是通过它来相互发现，实现互相通信的

比方说 哨兵1把自己的IP（172.16.19.3）和端口（26579）发布到 *__sentinel__:hello* 频道上，哨兵2和3订阅了该频道。那么此时，哨兵2和3就可以从这个频道直接获取哨兵1的IP地址和端口号。

然后，哨兵2、3可以和哨兵1建立网络连接。通过这个方式，哨兵2和3也可以建立网络连接，这样一来，哨兵集群就形成了

=== 哨兵如何得知从库的地址

哨兵向主库发送INFO命令来完成的。
哨兵给主库发送INFO命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控

IMPORTANT: 哨兵不能只和主、从库连接。因为，主从库切换后，客户端也需要知道新主库的连接信息，才能向新主库发送请求操作。所以，哨兵还需要完成把新主库的信息告诉客户端的任务

=== 基于pub/sub机制的客户端事件通知

* 哨兵就是一个运行在特定模式下的Redis实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务

* 每个哨兵实例也提供pub/sub机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件

.Table 某些重要频道
|===
|事件|频道
|主库下线事件| +sdown (实例进入 *主观下线* 状态)
|主库下线事件| -sdown (实例退出 *主观下线* 状态)
|主库下线事件| +odown (实例进入 *客观下线* 状态)
|主库下线事件| -sdown (实例退出 *客观下线* 状态)
|从库重新配置事件| +slave-reconf-sent (哨兵发送 *slaveof* (*replicaof*)命令重新配置入库)
|从库重新配置事件| +slave-reconf-inprog (从库配置了新主库，但尚未进行同步)
|从库重新配置事件| +slave-reconf-done (从库配置了新主库，且和新主库完成同步)
|新主库切换| +switch-master (主库地址发生了变化)
|===

* 可以用客户端去订阅这些频道。获取主从切换的过程和进度

=== 哨兵如何决定由谁去执行主从切换

* 任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 *is-master-down-by-addr* 命令

* 其他实例会根据自己和主库的连接情况，做出Y或N的响应，Y相当于赞成票，N相当于反对票。

* 一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 *quorum* 配置项设定的 (哨兵在发起投票前肯定会给自己投票)

* 哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader选举”。

* 因为最终执行主从切换的哨兵称为Leader，投票过程就是确定Leader, 以下是选主成功需要达成的条件

** 拿到半数以上的赞成票 (活着的节点)

** 拿到的票数同时还需要大于等于哨兵配置文件中的 *quorum* 值

* 如果哨兵集群只有2个实例，此时，一个哨兵要想成为Leader，必须获得2票，而不是1票。所以，如果有个哨兵挂掉了，那么，此时的集群是无法进行主从库切换的。因此，通常我们至少会配置3个哨兵实例

== 一些额外的问题

=== 哨兵集群故障时的选主情况

假设有一个Redis集群，是“一主四从”，同时配置了包含5个哨兵实例的集群，quorum值设为2。在运行过程中，如果有3个哨兵实例都发生故障了，此时，Redis主库如果有故障，还能正确地判断主库“客观下线”吗？如果可以的话，还能进行主从库自动切换吗

* 哨兵集群可以判定主库“主观下线”。由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线"

* 但哨兵不能完成主从切换。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到多数选票的结果

=== 哨兵实例是不是越多越好

哨兵在判定“主观下线”和选举“哨兵领导者”时，都需要和其他节点进行通信，交换信息，哨兵实例越多，通信的次数也就越多，而且部署多个哨兵时，会分布在不同机器上，节点越多带来的机器故障风险也会越大，这些问题都会影响到哨兵的通信和选举，出问题时也就意味着选举时间会变长，切换主从的时间变久

=== 调大down-after-milliseconds值，对减少误判是不是有好处

是有好处的，适当调大down-after-milliseconds值，当哨兵与主库之间网络存在短时波动时，可以降低误判的概率。但是调大down-after-milliseconds值也意味着主从切换的时间会变长，对业务的影响时间越久，我们需要根据实际场景进行权衡，设置合理的阈值





